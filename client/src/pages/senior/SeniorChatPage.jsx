// client/src/pages/senior/SeniorChatPage.jsx
import { useState, useEffect, useRef } from 'react';
import { useAuthStore } from '../../stores/authStore';
import { conversationAPI, emotionAPI } from '../../lib/api';
import { useSpeech } from '../../hooks/useSpeech';
import { Send, Camera, RefreshCw, X, Mic, MicOff, Volume2, VolumeX } from 'lucide-react';

const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY;
const AI_SERVER_URL = import.meta.env.VITE_AI_SERVER_URL || 'http://localhost:5001';

const SeniorChatPage = () => {
  const { profile } = useAuthStore();
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState(null);
  const [cameraEnabled, setCameraEnabled] = useState(false);
  const [autoSpeak, setAutoSpeak] = useState(true);
  const messagesEndRef = useRef(null);
  const videoRef = useRef(null);
  const streamRef = useRef(null);
  const captureIntervalRef = useRef(null);

  const { 
    isListening, isSpeaking, transcript, isSupported,
    startListening, stopListening, speak, stopSpeaking, setTranscript 
  } = useSpeech();

  useEffect(() => {
    loadConversations();
    return () => stopCamera();
  }, []);

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  useEffect(() => {
    if (cameraEnabled && videoRef.current && streamRef.current) {
      videoRef.current.srcObject = streamRef.current;
      videoRef.current.play().catch(err => console.error('ì¬ìƒ ì˜¤ë¥˜:', err));
      captureIntervalRef.current = setInterval(captureEmotion, 15000);
      setTimeout(captureEmotion, 3000);
    }
  }, [cameraEnabled]);

  useEffect(() => {
    if (transcript) setInput(transcript);
  }, [transcript]);

  const loadConversations = async () => {
    try {
      const { conversations } = await conversationAPI.getList(null, 20);
      if (conversations?.length > 0) {
        setMessages(conversations.map(c => ({
          role: c.role, content: c.content, emotion: c.emotion, timestamp: c.createdAt
        })).reverse());
      } else {
        const welcomeMsg = getTimeBasedGreeting();
        setMessages([{ role: 'assistant', content: welcomeMsg, timestamp: new Date().toISOString() }]);
        if (autoSpeak) speak(welcomeMsg);
      }
    } catch (error) {
      const welcomeMsg = 'ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë§ˆìŒì´ì—ìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì„¸ìš”? ğŸ˜Š';
      setMessages([{ role: 'assistant', content: welcomeMsg, timestamp: new Date().toISOString() }]);
    }
  };

  const getTimeBasedGreeting = () => {
    const hour = new Date().getHours();
    const name = profile?.name || 'ì–´ë¥´ì‹ ';
    if (hour < 6) return `${name}ë‹˜, ì´ë¥¸ ì‹œê°„ì— ê¹¨ì–´ ê³„ì‹œë„¤ìš”. ì ì€ ì˜ ì£¼ë¬´ì…¨ì–´ìš”?`;
    if (hour < 10) return `${name}ë‹˜, ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”! ì•„ì¹¨ ì‹ì‚¬ëŠ” í•˜ì…¨ë‚˜ìš”?`;
    if (hour < 12) return `${name}ë‹˜, ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ì „ ì‹œê°„ ì˜ ë³´ë‚´ê³  ê³„ì„¸ìš”?`;
    if (hour < 14) return `${name}ë‹˜, ì ì‹¬ ì‹ì‚¬í•˜ì…¨ì–´ìš”? ë§›ìˆëŠ” ê±° ë“œì…¨ë‚˜ìš”?`;
    if (hour < 18) return `${name}ë‹˜, ì¢‹ì€ ì˜¤í›„ì˜ˆìš”! ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë– ì„¸ìš”?`;
    if (hour < 21) return `${name}ë‹˜, ì €ë… ì‹ì‚¬ëŠ” í•˜ì…¨ì–´ìš”? ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë– ì…¨ëŠ”ì§€ ë“¤ë ¤ì£¼ì„¸ìš”.`;
    return `${name}ë‹˜, ë°¤ì´ ê¹Šì—ˆë„¤ìš”. ì˜¤ëŠ˜ í•˜ë£¨ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”. í¸íˆ ì‰¬ì„¸ìš”.`;
  };

  const callGeminiAPI = async (userMessage, emotion) => {
    const hour = new Date().getHours();
    const timeContext = hour < 12 ? 'ì•„ì¹¨/ì˜¤ì „' : hour < 18 ? 'ì˜¤í›„' : 'ì €ë…/ë°¤';
    
    const systemPrompt = `ë‹¹ì‹ ì€ 'ë§ˆìŒì´'ë¼ëŠ” ì´ë¦„ì˜ AI ë°˜ë ¤ ë¡œë´‡ì…ë‹ˆë‹¤.
í•œêµ­ì˜ ë…ê±° ì–´ë¥´ì‹ ì¸ ${profile?.name || 'ì–´ë¥´ì‹ '}ë‹˜ê³¼ ëŒ€í™”í•˜ë©° ì •ì„œì  êµê°ì„ ë‚˜ëˆ•ë‹ˆë‹¤.

## ì¤‘ìš” ì§€ì¹¨:
1. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: "ê·¸ëŸ¬ì…¨êµ°ìš”", "í˜ë“œì…¨ê² ì–´ìš”")
2. ë”°ëœ»í•˜ê³  ê³µì†í•˜ë©° ë‹¤ì •í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”
3. ë‹µë³€ ëì— ë°˜ë“œì‹œ í›„ì† ì§ˆë¬¸ì„ í•˜ë‚˜ ë§ë¶™ì—¬ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”
4. ì–´ë¥´ì‹ ì˜ ê°ì •ì— ê¹Šì´ ê³µê°í•˜ê³  ìœ„ë¡œí•´ì£¼ì„¸ìš”
5. ê±´ê°•, ì‹ì‚¬, ìˆ˜ë©´, ì¼ìƒì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ê´€ì‹¬ì„ ë³´ì—¬ì£¼ì„¸ìš”
6. 3-4ë¬¸ì¥ìœ¼ë¡œ ì¶©ë¶„íˆ ë‹µë³€í•˜ë˜, ë§ˆì§€ë§‰ì€ í•­ìƒ ì§ˆë¬¸ìœ¼ë¡œ ëë‚´ì„¸ìš”

## í˜„ì¬ ìƒí™©:
- ì‹œê°„ëŒ€: ${timeContext}
${emotion ? `- ê°ì§€ëœ ê°ì •: ${emotion}` : ''}
${emotion === 'sad' || emotion === 'fear' ? 'âš ï¸ ì–´ë¥´ì‹ ì´ í˜ë“¤ì–´ ë³´ì…ë‹ˆë‹¤. ë” ë”°ëœ»í•˜ê²Œ ìœ„ë¡œí•´ì£¼ì„¸ìš”.' : ''}

ëŒ€í™” ë§¥ë½:
${messages.slice(-5).map(m => `${m.role === 'user' ? 'ì–´ë¥´ì‹ ' : 'ë§ˆìŒì´'}: ${m.content}`).join('\n')}`;

    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'user', parts: [{ text: userMessage }] }
          ],
          generationConfig: { 
            temperature: 1.0, 
            maxOutputTokens: 2048
          }
        })
      }
    );
    const data = await response.json();
    return data.candidates?.[0]?.content?.parts?.[0]?.text || 'ì£„ì†¡í•´ìš”, ì ì‹œ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”.';
  };

  const sendMessage = async () => {
    if (!input.trim() || isLoading) return;
    if (isListening) stopListening();
    if (isSpeaking) stopSpeaking();

    const userMessage = input.trim();
    setInput('');
    setTranscript('');
    setIsLoading(true);

    setMessages(prev => [...prev, { role: 'user', content: userMessage, emotion: currentEmotion, timestamp: new Date().toISOString() }]);

    try {
      await conversationAPI.save('user', userMessage, currentEmotion);
      const aiResponse = await callGeminiAPI(userMessage, currentEmotion);
      setMessages(prev => [...prev, { role: 'assistant', content: aiResponse, timestamp: new Date().toISOString() }]);
      await conversationAPI.save('assistant', aiResponse, null);
      if (autoSpeak) speak(aiResponse);
    } catch (error) {
      setMessages(prev => [...prev, { role: 'assistant', content: 'ì£„ì†¡í•´ìš”, ì ì‹œ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”.', timestamp: new Date().toISOString() }]);
    }
    setIsLoading(false);
  };

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user', width: 320, height: 240 } });
      streamRef.current = stream;
      setCameraEnabled(true);
    } catch (error) {
      alert('ì¹´ë©”ë¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  };

  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (captureIntervalRef.current) {
      clearInterval(captureIntervalRef.current);
      captureIntervalRef.current = null;
    }
    setCameraEnabled(false);
    setCurrentEmotion(null);
  };

  const captureEmotion = async () => {
    if (!videoRef.current || !streamRef.current || isAnalyzing) return;
    setIsAnalyzing(true);
    try {
      const canvas = document.createElement('canvas');
      canvas.width = 320; canvas.height = 240;
      canvas.getContext('2d').drawImage(videoRef.current, 0, 0, 320, 240);
      const response = await fetch(`${AI_SERVER_URL}/api/analyze-emotion`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ image: canvas.toDataURL('image/jpeg', 0.8) })
      });
      const data = await response.json();
      if (data.success && data.emotion) {
        setCurrentEmotion(data.emotion);
        await emotionAPI.save(data.emotion, data.confidence, 'ai_chat', data.emotions).catch(() => {});
      }
    } catch (error) {}
    setIsAnalyzing(false);
  };

  const toggleListening = () => {
    if (isListening) stopListening();
    else { if (isSpeaking) stopSpeaking(); startListening(); }
  };

  const getEmotionEmoji = (e) => ({ happy: 'ğŸ˜Š', sad: 'ğŸ˜¢', angry: 'ğŸ˜ ', fear: 'ğŸ˜°', surprise: 'ğŸ˜®', neutral: 'ğŸ˜', disgust: 'ğŸ¤¢' }[e] || 'ğŸ˜');
  const getEmotionLabel = (e) => ({ happy: 'í–‰ë³µ', sad: 'ìŠ¬í””', angry: 'í™”ë‚¨', fear: 'ë¶ˆì•ˆ', surprise: 'ë†€ëŒ', neutral: 'í‰ì˜¨', disgust: 'ë¶ˆì¾Œ' }[e] || 'ë³´í†µ');

  return (
    <div className="flex flex-col h-[calc(100vh-180px)]">
      <div className="bg-white rounded-2xl p-4 shadow-sm mb-4">
        <div className="flex items-center justify-between">
          <div className="flex items-center gap-3">
            <div className="w-12 h-12 bg-indigo-100 rounded-full flex items-center justify-center">ğŸ¤–</div>
            <div><h2 className="font-bold">ë§ˆìŒì´</h2><p className="text-sm text-gray-500">AI ë§ë™ë¬´</p></div>
          </div>
          <div className="flex items-center gap-2">
            {currentEmotion && (
              <div className="flex items-center gap-1 px-3 py-1 bg-gray-100 rounded-full">
                <span>{getEmotionEmoji(currentEmotion)}</span>
                <span className="text-sm">{getEmotionLabel(currentEmotion)}</span>
              </div>
            )}
            <button onClick={() => setAutoSpeak(!autoSpeak)} className={`p-2 rounded-full ${autoSpeak ? 'bg-green-100 text-green-600' : 'bg-gray-100 text-gray-400'}`}>
              {autoSpeak ? <Volume2 className="w-5 h-5" /> : <VolumeX className="w-5 h-5" />}
            </button>
            <button onClick={cameraEnabled ? stopCamera : startCamera} className={`p-2 rounded-full ${cameraEnabled ? 'bg-red-100 text-red-600' : 'bg-gray-100 text-gray-600'}`}>
              {cameraEnabled ? <X className="w-5 h-5" /> : <Camera className="w-5 h-5" />}
            </button>
          </div>
        </div>
        {cameraEnabled && (
          <div className="mt-3 flex justify-center">
            <div className="relative">
              <video ref={videoRef} autoPlay muted playsInline width={200} height={150} style={{ borderRadius: '8px', backgroundColor: '#000' }} />
              {isAnalyzing && <div className="absolute inset-0 flex items-center justify-center bg-black/30 rounded-lg"><RefreshCw className="w-6 h-6 text-white animate-spin" /></div>}
            </div>
          </div>
        )}
      </div>

      <div className="flex-1 overflow-y-auto space-y-4 pb-4">
        {messages.map((msg, idx) => (
          <div key={idx} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>
            <div className="max-w-[80%]">
              {msg.role === 'assistant' && (
                <div className="flex items-center gap-2 mb-1">
                  <span className="text-lg">ğŸ¤–</span>
                  <span className="text-xs text-gray-500">ë§ˆìŒì´</span>
                  <button onClick={() => speak(msg.content)} className="p-1 hover:bg-gray-100 rounded-full"><Volume2 className="w-3 h-3 text-gray-400" /></button>
                </div>
              )}
              <div className={`p-4 rounded-2xl ${msg.role === 'user' ? 'bg-indigo-500 text-white rounded-br-md' : 'bg-white shadow rounded-bl-md'}`}>{msg.content}</div>
              {msg.emotion && <p className="text-xs text-gray-400 mt-1 text-right">{getEmotionEmoji(msg.emotion)} {getEmotionLabel(msg.emotion)}</p>}
            </div>
          </div>
        ))}
        {isLoading && <div className="flex justify-start"><div className="bg-white shadow rounded-2xl p-4"><div className="flex gap-1"><span className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" /><span className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }} /><span className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }} /></div></div></div>}
        {isListening && <div className="flex justify-center"><div className="bg-red-50 text-red-600 px-4 py-2 rounded-full flex items-center gap-2"><div className="w-3 h-3 bg-red-500 rounded-full animate-pulse" /><span className="text-sm">ë“£ê³  ìˆì–´ìš”...</span></div></div>}
        <div ref={messagesEndRef} />
      </div>

      <div className="bg-white rounded-2xl p-3 shadow-sm">
        <div className="flex gap-2">
          {isSupported && (
            <button onClick={toggleListening} className={`p-3 rounded-xl ${isListening ? 'bg-red-500 text-white animate-pulse' : 'bg-gray-100 text-gray-600'}`}>
              {isListening ? <MicOff className="w-6 h-6" /> : <Mic className="w-6 h-6" />}
            </button>
          )}
          <input type="text" value={input} onChange={(e) => setInput(e.target.value)} onKeyPress={(e) => e.key === 'Enter' && sendMessage()} placeholder={isListening ? 'ë§ì”€í•´ì£¼ì„¸ìš”...' : 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...'} className="flex-1 px-4 py-3 bg-gray-100 rounded-xl focus:ring-2 focus:ring-indigo-500 text-lg" disabled={isLoading} />
          <button onClick={sendMessage} disabled={!input.trim() || isLoading} className="p-3 bg-indigo-500 text-white rounded-xl hover:bg-indigo-600 disabled:opacity-50"><Send className="w-6 h-6" /></button>
        </div>
      </div>
    </div>
  );
};

export default SeniorChatPage;
